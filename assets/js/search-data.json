{
  
    
        "post0": {
            "title": "Show Me the City Data",
            "content": ". In this Post . Let&#39;s bring some first post energy! I am excited to kick off this blog with a post about accessing Open Data in an easy, programmatic manner. . This notebook will demonstrate how to programatically read in Open Data from Socrata databases. Socrata is a company commissioned by a large number of cities to host open data platforms. . These Open Data platforms are searchable and most importantly the data can be queried via an API. These queries can be quite simple, or more complex. We will dive into examples of both and be querying data in no time! . Why is this Important . Querying and accessing data is a fundamental step of any data science workflow. The mechanics of accessing data can get very messy, especially when it comes to government data. Examples of this messiness include manually downloading files and saving to a user-defined location, appending multiple Excel files, scraping PDFs, and the list goes on. Bottom line: it can get MESSY. . Luckily, the infrastructure provided by Socrata makes for a seamless experience that can be replicated by anyone with the Internet and Python. You read that correctly, everything demonstrated in this post can be replicated with very little setup. This is a huge perk for the sake of collaboration. Additionally, in the event that an analysis needs to be re-run, perhaps on more recent data, the ability to query data programatically allows for minimal rework and minimal room for error. Alright, enough hype let&#39;s get to it! . Setup . Before we get rolling, it is important to be working in an environment with the necessary packages installed and available. I like using Conda to manage environments (this sounds like a future blog post) and for the demonstration below it will be important to have pandas installed in your environment. . import pandas as pd . ModuleNotFoundError Traceback (most recent call last) &lt;ipython-input-1-45fa858dc579&gt; in &lt;module&gt; 1 # load pandas -&gt; 2 import pandas as pd ModuleNotFoundError: No module named &#39;pandas&#39; . Finding an API Endpoint . Before we dive into actually reading data, it is important to know where to look for potential datasets. Most cities have their Open Data tied to the city&#39;s website. In this case we are going to explore the City of Cincinnati&#39;s Open Data Portal, which can be found at this link. . . At this point, we can navigate into one of the categories or search for a specific dataset. In this case, we select the &#39;Safety&#39; category and will then choose the Police Calls dataset. . . We are now able to Copy the API Endpoint by selecting the &#39;API&#39; button and the &#39;Copy&#39; button next to the endpoint. . . Reading in Data via API . You might notice that the API endpoint is just a link - try accessing the link below in your browser. You will notice that the result is just a JSON string in your browser - this is the data we are pulling in imminently! . In order to read this into a pandas DataFrame, we utilize the pandas read_json function. This function only has one required argument, path_or_buf and we set the value to our API endpoint. More information on read_json can be found by looking at its docstring (simply run ?read_json). . Note that in this initial read, we only pull back 1,000 total rows. This is the default response when no other parameters are added to the endpoint URL. . crime_data_endpoint = &#39;https://data.cincinnati-oh.gov/resource/k59e-2pvf.json&#39; pd.read_json(path_or_buf = crime_data_endpoint) . Socrata Query Language (SoQL) . In the above example, we simply query the entire dataset (albeit a sample of 1,000 rows) given we did not make any tweaks to the base URL. However, we are able to arrive at fairly robust queries using the SoQL querying language. SoQL is a querying language developed by Socrata in order to make the process of requesting data via their API more efficient. In their documentation they mention their intentions of mirroring the spirit of SQL and I believe they accomplished that goal quite well. . Being well versed in this querying language provides enormous benefits in the case of processing time. It is far more efficient to execute data processing against the Socrata servers prior to bringing in data to your local machine. For the remainder of this post we will explore examples of utilizing SoQL queries to refine our data before pulling it into our local machines. . Example 1: Simple Filter . In our first example, we are going to execute a simple filter on our data by updating the $WHERE parameter. As you would imagine, the default value for the $WHERE parameter is No Filter. . In our initial query above, we see that there is an offense column with a number of different values, stored as strings. For this exercise, we are going to filter to instances where offense is equal to &#39;THEFT&#39;. . In practice, we are able to execute this by modifying our API endpoint with the simple filter clause stamped on at the end. To signal that we are going to add a SoQL clause to our endpoint, we first add a question mark (?) at the end of the initial endpoint, immediately following &#39;.json&#39;: https://data.cincinnati-oh.gov/resource/k59e-2pvf.json? . Now, we must add the actual where clause. In this case, we first declare that we are updating the $WHERE parameter by adding where=. We now add the filter statement, in this case offense = &#39;THEFT&#39;. This gives us a final query that looks like the following: . https://data.cincinnati-oh.gov/resource/k59e-2pvf.json?$WHERE=offense=&#39;THEFT&#39;. . Note: from this point forward we will simply reference crime_data_endpoint when generating queries. We will take advantage of f-String formatting that allows us to pass through a variable to a string statement (i.e. if name is a variable defined in the environment as the value of &amp;#8217;Joe&amp;#8217;, the statement f&quot;I am {name}&quot; will evaluate to &amp;#8217;I am Joe&amp;#8217;. . pd.read_json(f&quot;{crime_data_endpoint}?$WHERE=offense=&#39;THEFT&#39;&quot;) . With the query above, we see the data returned is exclusively limited to &#39;THEFT&#39; offenses as desired. We are now going to demonstrate the ability to string multiple WHERE clauses into the same query. . In order to do so, we tap into the logical operators allowed within the SoQL framework. The following operators are available as illustrated in the documentation: . AND (&amp;) | OR (|) | NOT | IS NULL | IS NOT NULL | . In this case, we are going to add the condition that an incident must have occurred in the 45202 zip code. To accomplish this, we simply add the &amp; symbol and an additional condition of zip=45202. . Note: the SoQL documentation shows examples that include spaces in the endpoint URL. The pd.read_json() function does not accept URLs with spaces. Do your best to avoid using spaces, or replace the space with its URL encoding (in the case of a space that is %20). . pd.read_json(f&quot;{crime_data_endpoint}?$WHERE=offense=&#39;THEFT&#39;&amp;zip=45202&quot;) . Example 2: Date Filter . Working with dates can be finnicky in any scenario. In this example, we aim to illustrate how to introduce a simple filter to limit the date range of our returned crime instances. . As you can see and infer from above, we have several fields that are represented as timestamps. For this exercise, we are going to specifically focus on the date_reported field. Let&#39;s explore how we would only look at records for which the date_reported value occurred within the last year. . For this exercise we will take advantage of the datetime library that assists with time and date operations. . import datetime as dt # dynamically generate today&#39;s date today = dt.date.today() # dynamically arrive at the date exactly 365 days ago year_ago_today = today - dt.timedelta(days = 365) print(year_ago_today) . Now that we have our date for exactly one year ago, we can utilize it within our API call. Similar to the simple filters above, we simply create a $WHERE clause, but in this case we look at instances where the date_reported field satisfies the condition of being after our year_ago_today value. . Note: it is important to enclose the value for year_ago_today in quotes. In this case I use single quotes for within the query, and enclose the entire query using double quotes. . pd.read_json(f&quot;{crime_data_endpoint}?$WHERE=date_reported&gt;=&#39;{year_ago_today}&#39;&quot;) . Example 3: Toggling Row Limit . By default, the row limit returned by the Socrata API is 1,000. This is great for datasets smaller than 1,000 rows and illustrative blog posts, but for anything that actually matters it will be woefully insufficient. Hence, we will now walk through the means for adjusting the row limit and integrating this into our queries. . This mechanic is really quite straightforward. Similar to other SoQL query arguments, we simply define the parameter we are adjusting by adding $LIMIT, and then specifying the value for which to limit the row counts. In the example below we limit this to 10, however, you could ramp it up as large or small as needed. . Note: I have not found an &#8217;unlimited&#8217; value for this argument, thus if I know I would like to return all records I simply pass through an uncharacteristically large value that would not be anywhere near the max size of the data. This is not a perfect solution but gets the job done when you know the rough size of your data. . pd.read_json(f&quot;{crime_data_endpoint}?$LIMIT=10&quot;) . pd.read_json(f&quot;{crime_data_endpoint}?$WHERE=offense=&#39;THEFT&#39;&amp;$LIMIT=5&quot;) . Example 4: Select Subset of Columns . In this example we walkthrough the method for selecting only a subset of the columns in a dataframe. This improves performance when you only need to work with a subset of columns by reducing the size of the dataframe. . The syntax is fairly straightforward and mirrors the methods for adding other parameters to our query. Simply add a $SELECT= clause to the URL and list columns to include, separated by commas. . Note: again, it is important to avoid spaces within the URL. There is functionality to select columns under new names, like so: $SELECT=suspect_gender AS gender. However, in this case we need to clean up the query to avoid any spaces in the URL by replacing them with the %20 notation: $SELECT=suspect_gender%20AS%20gender. . pd.read_json(f&quot;{crime_data_endpoint}?$SELECT=incident_no,date_reported,suspect_gender&quot;) . Example 5: Putting it All Together with $query . Up until this point, we have walked through the parameters that can be modified within a query on an individual basis. It was noted that multiple parameters can be combined by adding an &amp; symbol between parameters, leading to more complex queries. However, there is an easier way to string these parameters together, and it manifests itself within the $query parameter. . In these examples we will demonstrate how to string together a full-fledged query using the single $query parameter. These queries will strongly resemble standard SQL queries that lack a FROM clause. . To make things simpler, I have created a function (generate_query) to take care of some of the less readable pieces of these queries, notably the requirement to replace spaces (&#39; &#39;) with &#39;%20&#39;. Furthermore, this function breaks the components of the query into chunks, making it very human readable. . # function returns a full, cleaned up API call def generate_query(endpoint_url, select_clause, where_clause, group_clause, order_by_clause): raw_query = (f&quot;{endpoint_url}?$query=&quot; f&quot;{select_clause}%20&quot; f&quot;{where_clause}%20&quot; f&quot;{group_clause}%20&quot; f&quot;{order_by_clause}&quot; ) final_query = str.replace(raw_query, &quot; &quot;, &quot;%20&quot;) return final_query . Now, with our function in hand let&#39;s accomplish the following within two separate queries: . Select the columns date_reported and offense for records where suspect_gender is &#39;MALE&#39; | Count the number of records by offense type for all crimes within the past year, order by the count in descending order | first_query = generate_query(endpoint_url = crime_data_endpoint, select_clause = &quot;select date_reported, offense&quot;, where_clause = &quot;where offense = &#39;THEFT&#39;&quot;, group_clause = &quot;&quot;, order_by_clause = &quot;&quot;) print(first_query) . pd.read_json(first_query) . ## order by the count in descending order second_query = generate_query(endpoint_url = crime_data_endpoint, select_clause = &quot;select offense, count(*) as n&quot;, where_clause = f&quot;where date_reported&gt;=&#39;{year_ago_today}&#39;&quot;, group_clause = &quot;group by offense&quot;, order_by_clause = &quot;order by count(*) desc&quot; ) print(second_query) . pd.read_json(second_query) . And with that we conclude the first post of the blog! I hope you found this information helpful and I can&#39;t wait to deliver more content that explores the intersection of cities and data science. .",
            "url": "https://jmbahr.github.io/the-city-scraper/jupyter/2021/08/19/read-socrata.html",
            "relUrl": "/jupyter/2021/08/19/read-socrata.html",
            "date": " • Aug 19, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://jmbahr.github.io/the-city-scraper/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://jmbahr.github.io/the-city-scraper/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". My name is Joe Bahr, and I am a Master’s candidate at Duke University’s Sanford School of Public Policy. . After working for four years in grocery retail as a data scientist, I am excited to explore the world of data science in the context of policy. . Since entering the industry, the tools and methods available to data scientists has grown enormously. Furthermore, I have spent time dabbling with the publicly available data published by cities and am enthused by the amount of data available. This combination is truly exciting, and I hope to share that enthusiasm via this blog. .",
          "url": "https://jmbahr.github.io/the-city-scraper/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jmbahr.github.io/the-city-scraper/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}